@article{Quick2014,
abstract = {Cloud storage is an emerging challenge to digital forensic examiners. The services are increasingly used by consumers, business, and government, and can potentially store large amounts of data. The retrieval of digital evidence from cloud storage services (particularly from offshore providers) can be a challenge in a digital forensic investigation, due to virtualisation, lack of knowledge on location of digital evidence, privacy issues, and legal or jurisdictional boundaries. Google Drive is a popular service, providing users a cost-effective, and in some cases free, ability to access, store, collaborate, and disseminate data. Using Google Drive as a case study, artefacts were identified that are likely to remain after the use of cloud storage, in the context of the experiments, on a computer hard drive and Apple iPhone3G, and the potential access point(s) for digital forensics examiners to secure evidence. {\textcopyright} 2013 Elsevier Ltd.},
author = {Quick, Darren and Choo, Kim Kwang Raymond},
doi = {10.1016/j.jnca.2013.09.016},
file = {:C$\backslash$:/Users/Hp/Desktop/Hp Backup/Downloads/ApacheSpark/Google Drive  Forensic analysis of data remnants.pdf:pdf},
isbn = {9780124199705},
issn = {10958592},
journal = {Journal of Network and Computer Applications},
keywords = {Cloud forensics,Cloud storage,Cloud storage forensics,Digital forensic analysis,Google Drive},
number = {1},
pages = {179--193},
publisher = {Elsevier},
title = {{Google drive: Forensic analysis of data remnants}},
url = {http://dx.doi.org/10.1016/j.jnca.2013.09.016},
volume = {40},
year = {2014}
}
@article{Birjali2017,
abstract = {Social Media provides organizations ability to survey feelings towards the contents and events associated to them in real time. Moreover, the first demarche of the sentiment analysis is the pre-processing of data collected from Social Media. Most of existing research works that deals with social media analysis based on extracting new features related to sentiment. This paper presents the usage of Twitter in a number of proposed subjects, which is the largest social networking website where Twitter data is in increasing at higher rates every day that considers it as Big Data Source. Then, describing in detail the way in which Big data technology, such as, InfoSphere BigInsights enables processing of this data, which are primarily collected from social networks by Apache Flume and stored in Hadoop storage. In addition, we have investigated a Big Data platform for collecting social media data based on Apache Flume and analyzing this data using InfoSphere BigInsights. Moreover, our paper integrates the visualization of these analysis results using BigSheets. To that end, evaluation through analysis of results confirms that the proposed Big Data platform produces better results in terms of social media analysis. Peer-review under responsibility of the Conference Program Chairs.},
author = {Birjali, Marouane and Beni-Hssane, Abderrahim and Erritali, Mohammed},
doi = {10.1016/j.procs.2017.08.299},
file = {:C$\backslash$:/Users/Hp/Desktop/Hp Backup/Downloads/ApacheSpark/Analyzing Social Media through Big Data using InfoSphere.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Apache Flume,Big Data,BigSheets,Hadoop,Infosphere BigInsights,Social Media Analysis,Twitter Data},
pages = {280--285},
publisher = {Elsevier B.V.},
title = {{Analyzing Social Media through Big Data using InfoSphere BigInsights and Apache Flume}},
url = {http://dx.doi.org/10.1016/j.procs.2017.08.299},
volume = {113},
year = {2017}
}
@article{DeFarias2017,
abstract = {This paper presents COMFIT (Cloud and Model based IDE for the Internet of Things), a development environment for the Internet of Things that was built grounded on the paradigms of model driven development and cloud computing. COMFIT is composed of two different modules: (1) the App Development Module, a model-driven architecture (MDA) infrastructure, and (2) the App Management and Execution Module, a module that contains cloud-based web interface connected to a server hosted in the cloud with compilers and simulators for developing Internet of Things (IoT) applications. The App Development Module allows the developers to design IoT applications using high abstraction artifacts (models), which are tailored to either the application perspective or the network perspective, thus creating a separation between these two concerns. As models can be automatically transformed into code through the App Development Module, COMFIT creates an environment where there is no need of additional configurations to properly compile or simulate the generated code, integrating the development lifecycle of IoT applications into a single environment partially hosted in the client side and partially in the cloud. In its current version, COMFIT supports two operating systems, namely Contiki and TinyOS, which are widely used in IoT devices. COMFIT supports automatic code generation, execution of simulations, and code compilation of applications for these platforms with low development effort. Finally, COMFIT is able to interact with IoT-lab, an open testbed for IoT applications, which allows the developers to test their applications with different configurations without the need of using local IoT devices. Several evaluations were performed to assess COMFIT's key features in terms of development effort, quality of generated code, and scalability.},
author = {de Farias, Claudio M. and Brito, Italo C. and Pirmez, Luci and Delicato, Fl{\'{a}}via C. and Pires, Paulo F. and Rodrigues, Taniro C. and dos Santos, Igor L. and Carmo, Luiz F.R.C. and Batista, Thais},
doi = {10.1016/j.future.2016.06.031},
file = {:C$\backslash$:/Users/Hp/Desktop/Hp Backup/Downloads/ApacheSpark/A development environment for the Internet of Things.pdf:pdf},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Cloud computing,IDE,Internet of Things,Model-driven development,Testbeds,Wireless sensor networks},
pages = {128--144},
publisher = {Elsevier B.V.},
title = {{COMFIT: A development environment for the Internet of Things}},
url = {http://dx.doi.org/10.1016/j.future.2016.06.031},
volume = {75},
year = {2017}
}
@article{KacfahEmani2015,
abstract = {This survey presents the concept of Big Data. Firstly, a definition and the features of Big Data are given. Secondly, the different steps for Big Data data processing and the main problems encountered in big data management are described. Next, a general overview of an architecture for handling it is depicted. Then, the problem of merging Big Data architecture in an already existing information system is discussed. Finally this survey tackles semantics (reasoning, coreference resolution, entity linking, information extraction, consolidation, paraphrase resolution, ontology alignment) in the Big Data context.},
author = {{Kacfah Emani}, Cheikh and Cullot, Nadine and Nicolle, Christophe},
doi = {10.1016/j.cosrev.2015.05.002},
file = {:C$\backslash$:/Users/Hp/Desktop/Hp Backup/Downloads/ApacheSpark/Understandable big data - A survey.pdf:pdf},
isbn = {1574-0137},
issn = {15740137},
journal = {Computer Science Review},
keywords = {Big data,Coreference resolution,Entity linking,Hadoop,Information extraction,Ontology alignment,Reasoning},
pages = {70--81},
publisher = {Elsevier Inc.},
title = {{Understandable Big Data: A survey}},
url = {http://dx.doi.org/10.1016/j.cosrev.2015.05.002},
volume = {17},
year = {2015}
}
@article{Teing2017,
abstract = {Cloud computing can be generally regarded as the technology enabler for Internet of Things (IoT). To ensure the most effective collection of evidence from cloud-enabled IoT infrastructure, it is vital for forensic practitioners to possess a contemporary understanding of the artefacts from different cloud services and applications. In this paper, we seek to determine the data remnants from the use of the newer BitTorrent Sync applications (version 2.x). Findings from our research using mobile and computer devices running Windows, Mac OS, Ubuntu, iOS, and Android devices suggested that artefacts relating to the installation, uninstallation, log-in, log-off, and file synchronisation could be recovered, which are potential sources of IoT forensics. We also extend the cloud forensics framework of Martini and Choo to provide a forensically sound investigation methodology for the newer BitTorrent Sync applications.},
author = {Teing, Yee Yang and Dehghantanha, Ali and Choo, Kim Kwang Raymond and Yang, Laurence T.},
doi = {10.1016/j.compeleceng.2016.08.020},
file = {:C$\backslash$:/Users/Hp/Desktop/Hp Backup/Downloads/ApacheSpark/Forensic investigation of P2P cloud storage services and backbone for IoT networks.pdf:pdf},
issn = {00457906},
journal = {Computers and Electrical Engineering},
keywords = {BitTorrent Sync,Cloud forensics,Computer forensics,Internet of things forensics,Mobile forensics,P2p cloud investigation},
number = {2017},
pages = {350--363},
publisher = {Elsevier Ltd},
title = {{Forensic investigation of P2P cloud storage services and backbone for IoT networks: BitTorrent Sync as a case study}},
volume = {58},
year = {2017}
}
@article{Gao2017,
author = {Gao, Yuanzhao and Li, Binglong},
doi = {10.1007/s11859-017-1224-7},
file = {:C$\backslash$:/Users/Hp/Desktop/Assignment Stuff/A forensic mathod for efficient file extraction in HDFS Based on Three- level mapping.pdf:pdf},
issn = {10071202},
journal = {Wuhan University Journal of Natural Sciences},
keywords = {Ext4,cloud forensics,file extraction,file recovery,metadata,the Hadoop distributed file system (HDFS) forensics,three-level (3L) mapping},
number = {2},
pages = {114--126},
title = {{A forensic method for efficient file extraction in HDFS based on three-level mapping}},
volume = {22},
year = {2017}
}
@article{Jeong2013,
author = {Jeong, Jaein},
doi = {10.1109/RTCSA.2013.6732232},
file = {:C$\backslash$:/Users/Hp/Desktop/Assignment Stuff/High perfomance Logging System for embedded UNIX and GNULinux Application.pdf:pdf},
isbn = {978-1-4799-0850-9},
journal = {2013 IEEE 19th International Conference on Embedded and Real-Time Computing Systems and Applications, RTCSA 2013},
pages = {310--319},
title = {{High performance logging system for embedded UNIX and GNU/Linux applications}},
year = {2013}
}
@article{Sammons2015,
abstract = {Abstract Microsoft Windows is the most widely used operating system in the world. Thus, digital forensic examiners must have an understanding of how artifacts are created in Windows and how they can be used to track a user's activity. This chapter covers deleted data and artifacts such as restore points, metadata, the Recycle Bin, and more.},
author = {Sammons, John},
doi = {http://dx.doi.org/10.1016/B978-0-12-801635-0.00005-X},
file = {:C$\backslash$:/Users/Hp/Desktop/Assignment Stuff/Windows system artifacts.pdf:pdf},
isbn = {978-0-12-801635-0},
journal = {The Basics of Digital Forensics (Second Edition)},
keywords = {Deleted Data,Hiberfile.sys,Metadata,Most Recently Used (MRU),Print Spooling,Recycle Bin,Registry,Restore Points (RPs),Shadow Copies,Thumbnail Cache},
pages = {65--82},
title = {{Chapter 5 - Windows system artifacts}},
url = {http://www.sciencedirect.com/science/article/pii/B978012801635000005X},
year = {2015}
}
@article{Do2014,
abstract = {Event logs provide an audit trail that records user events and activities on a computer and are a potential source of evidence in digital forensic investigations. This paper presents a Windows event forensic process (WinEFP) for analyzing Windows operating system event log files. The WinEFP covers a number of relevant events that are encountered in Windows forensics. As such, it provides practitioners with guidance on the use of Windows event logs in digital forensic investigations.},
author = {Do, Quang and Martini, Ben and Looi, Jonathan and Wang, Yu and Choo, Kim-kwang},
doi = {10.1007/978-3-662-44952-3_7},
file = {:C$\backslash$:/Users/Hp/Desktop/Assignment Stuff/Windows event forensic process.pdf:pdf},
isbn = {1868-4238 978-3-662-44951-6},
issn = {18684238},
journal = {Advances in Digital Forensics X},
keywords = {windows event forensic process,windows event logs},
pages = {87--100},
title = {{Windows Event Forensic Process}},
url = {http://link.springer.com/10.1007/978-3-662-44952-3{\_}7},
volume = {433},
year = {2014}
}
@article{Roussev2013,
abstract = {There are two main reasons the processing speed of current generation digital forensic tools is inadequate for the average case: a) users have failed to formulate explicit performance requirements; and b) developers have failed to put performance, specifically latency, as a top-level concern in line with reliability and correctness. In this work, we formulate forensic triage as a real-time computation problem with specific technical requirements, and we use these requirements to evaluate the suitability of different forensic methods for triage purposes. Further, we generalize our discussion to show that the complete digital forensics process should be viewed as a (soft) real-time computation with well-defined performance requirements. We propose and validate a new approach to target acquisition that enables file-centric processing without disrupting optimal data throughput from the raw device. We evaluate core forensic processing functions with respect to processing rates and show their intrinsic limitations in both desktop and server scenarios. Our results suggest that, with current software, keeping up with a commodity SATA HDD at 120 MB/s requires 120-200 cores. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
author = {Roussev, Vassil and Quates, Candice and Martell, Robert},
doi = {10.1016/j.diin.2013.02.001},
file = {:C$\backslash$:/Users/Hp/Desktop/Hp Backup/Downloads/ApacheSpark/Real-time digital forensics and triage.pdf:pdf},
isbn = {1742-2876},
issn = {17422876},
journal = {Digital Investigation},
keywords = {Digital forensics,LOTA,Latency-optimized target acquisition,Real-time forensics,Triage},
number = {2},
pages = {158--167},
publisher = {Elsevier Ltd},
title = {{Real-time digital forensics and triage}},
url = {http://dx.doi.org/10.1016/j.diin.2013.02.001},
volume = {10},
year = {2013}
}
@article{Teing2016,
author = {Teing, Yee-yang and Sc, B and Dehghantanha, Ali and Ph, D and Choo, Kim-kwang Raymond and Ph, D},
doi = {10.1111/1556-4029.13271},
file = {::},
keywords = {among,arguably,cloud computing is,cloud forensics,cooperative cloud,digital forensics,discussed com-,due to its popularity,forensic science,mobile app forensics,mobile device forensics,one of the most,peer-to-peer forensics,puting paradigms in recent,symform analysis,years},
number = {May},
title = {{DIGITAL {\&} MULTIMEDIA SCIENCES Forensic Investigation of Cooperative Storage Cloud Service : Symform as a Case Study}},
year = {2016}
}
@article{Qian2015,
abstract = {Attribute reduction is one of the important research issues in rough set theory. Most existing attribute reduction algorithms are now faced with two challenging problems. On one hand, they have seldom taken granular computing into consideration. On the other hand, they still cannot deal with big data. To address these issues, the hierarchical encoded decision table is first defined. The relationships of hierarchical decision tables are then discussed under different levels of granularity. The parallel computations of the equivalence classes and the attribute significance are further designed for attribute reduction. Finally, hierarchical attribute reduction algorithms are proposed in data and task parallel using MapReduce. Experimental results demonstrate that the proposed algorithms can scale well and efficiently process big data.},
author = {Qian, Jin and Lv, Ping and Yue, Xiaodong and Liu, Caihui and Jing, Zhengjun},
doi = {10.1016/j.knosys.2014.09.001},
file = {:C$\backslash$:/Users/Hp/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Qian et al. - 2015 - Hierarchical attribute reduction algorithms for big data using MapReduce.pdf:pdf},
isbn = {0950-7051},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Big data,Data and task parallelism,Granular computing,Hierarchical attribute reduction,MapReduce},
pages = {18--31},
publisher = {Elsevier B.V.},
title = {{Hierarchical attribute reduction algorithms for big data using MapReduce}},
url = {http://dx.doi.org/10.1016/j.knosys.2014.09.001},
volume = {73},
year = {2015}
}
@article{Martini2012,
abstract = {Increasing interest in and use of cloud computing services presents both opportunities for criminal exploitation and challenges for law enforcement agencies (LEAs). For example, it is becoming easier for criminals to store incriminating files in the cloud computing environment but it may be extremely difficult for LEAs to seize these files as the latter could potentially be stored overseas. Two of the most widely used and accepted forensic frameworks - McKemmish (1999) and NIST (Kent et al., 2006) - are then reviewed to identify the required changes to current forensic practices needed to successfully conduct cloud computing investigations. We propose an integrated (iterative) conceptual digital forensic framework (based on McKemmish and NIST), which emphasises the differences in the preservation of forensic data and the collection of cloud computing data for forensic purposes. Cloud computing digital forensic issues are discussed within the context of this framework. Finally suggestions for future research are made to further examine this field and provide a library of digital forensic methodologies for the various cloud platforms and deployment models. {\textcopyright} 2012 Elsevier Ltd. All rights reserved.},
author = {Martini, Ben and Choo, Kim Kwang Raymond},
doi = {10.1016/j.diin.2012.07.001},
file = {:C$\backslash$:/Users/Hp/Desktop/Hp Backup/Downloads/ApacheSpark/An integrated conceptual digital forensic framework for cloud computing.pdf:pdf},
isbn = {1742-2876},
issn = {17422876},
journal = {Digital Investigation},
keywords = {Cloud computing,Cloud forensics,Computer forensics,Digital evidence,Digital forensics,Forensic computing},
number = {2},
pages = {71--80},
publisher = {Elsevier Ltd},
title = {{An integrated conceptual digital forensic framework for cloud computing}},
url = {http://dx.doi.org/10.1016/j.diin.2012.07.001},
volume = {9},
year = {2012}
}
@article{Gil2016,
abstract = {The term Big Data denotes huge-volume, complex, rapid growing datasets with numerous, autonomous and independent sources. In these new circumstances Big Data bring many attractive opportunities; however, good opportunities are always followed by challenges, such as modelling, new paradigms, novel architectures that require original approaches to address data complexities. The purpose of this special issue on Modeling and Management of Big Data is to discuss research and experience in modelling and to develop as well as deploy systems and techniques to deal with Big Data. A summary of the selected papers is presented, followed by a conceptual modelling proposal for Big Data. Big Data creates new requirements based on complexities in data capture, data storage, data analysis and data visualization. These concerns are discussed in detail in this study and proposals are recommended for specific areas of future research.},
author = {Gil, David and Song, Il-Yeol},
doi = {10.1016/j.future.2015.07.019},
file = {:C$\backslash$:/Users/Hp/Desktop/Hp Backup/Downloads/ApacheSpark/Modeling and Management of Big Data.pdf:pdf},
isbn = {0167739X},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {conceptual modeling big data},
pages = {96--99},
publisher = {Elsevier B.V.},
title = {{Modeling and Management of Big Data: Challenges and opportunities}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167739X15002514},
volume = {63},
year = {2016}
}
@article{Haggerty2008,
abstract = {A major advantage of information technology is the ease, speed and volume of information that may be shared between hosts. However, this has given rise to concerns over paedophile activity and the spread of malicious digital pictures amongst this community. In network forensic investigations a wealth of information relevant to the investigation will reside within the network itself and on disparate hosts. Current computer forensics tools are designed for the analysis of seized hard drives rather than investigating data within a network. In this paper we present FORWEB, a novel scheme for automated file fingerprinting of malicious pictures resident on Web servers. This approach may be used in forensic investigations to automatically identify repositories of malicious digital pictures on the Internet or to verify the Internet usage of a suspect. A case study and its results demonstrate the applicability of this approach.},
author = {Haggerty, John and Llewellyn-Jones, David and Taylor, Mark},
doi = {10.4108/e-forensics.2008.2774},
file = {:C$\backslash$:/Users/Hp/Desktop/Hp Backup/Downloads/ApacheSpark/FORWEB File Finger printing for authomated network forensic investigation.pdf:pdf},
isbn = {978-963-9799-19-6},
journal = {Proceedings of the 1st International ICST Conference on Forensic Applications and Techniques in Telecommunications, Information and Multimedia},
keywords = {Computer forensics,file fingerprinting,network investigations.},
pages = {6},
title = {{FORWEB: File Fingerprinting for Automated Network Forensics Investigations}},
url = {http://eudl.eu/doi/10.4108/e-forensics.2008.2774},
year = {2008}
}
@article{Kao2016,
abstract = {MapReduce is widely used in cloud applications for large-scale data processing. The increasing number of interactive cloud applications has led to an increasing need for MapReduce real-time scheduling. Most MapReduce applications are data-oriented and nonpreemptively executed. Therefore, the problem of MapReduce real-time scheduling is complicated because of the trade-off between run-time blocking for nonpreemptive execution and data-locality. This paper proposes a data-locality-aware MapReduce real-time scheduling framework for guaranteeing quality of service for interactive MapReduce applications. A scheduler and dispatcher that can be used for scheduling two-phase MapReduce jobs and for assigning jobs to computing resources are presented, and the dispatcher enable the consideration of blocking and data-locality. Furthermore, dynamic power management for run-time energy saving is discussed. Finally, the proposed methodology is evaluated by considering synthetic workloads, and a comparative study of different scheduling algorithms is conducted.},
author = {Kao, Yu Chon and Chen, Ya Shu},
doi = {10.1016/j.jss.2015.11.001},
file = {::},
isbn = {01641212},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Cloud computing systems,Data-locality,Real-time scheduling},
pages = {65--77},
publisher = {Elsevier Inc.},
title = {{Data-locality-aware mapreduce real-time scheduling framework}},
url = {http://dx.doi.org/10.1016/j.jss.2015.11.001},
volume = {112},
year = {2016}
}
@article{Sivarajah2017,
abstract = {Big Data (BD), with their potential to ascertain valued insights for enhanced decision-making process, have recently attracted substantial interest from both academics and practitioners. Big Data Analytics (BDA) is increasingly becoming a trending practice that many organizations are adopting with the purpose of constructing valuable information from BD. The analytics process, including the deployment and use of BDA tools, is seen by organizations as a tool to improve operational efficiency though it has strategic potential, drive new revenue streams and gain competitive advantages over business rivals. However, there are different types of analytic applications to consider. Therefore, prior to hasty use and buying costly BD tools, there is a need for organizations to first understand the BDA landscape. Given the significant nature of the BD and BDA, this paper presents a state-of-the-art review that presents a holistic view of the BD challenges and BDA methods theorized/proposed/employed by organizations to help others understand this landscape with the objective of making robust investment decisions. In doing so, systematically analysing and synthesizing the extant research published on BD and BDA area. More specifically, the authors seek to answer the following two principal questions: Q1 – What are the different types of BD challenges theorized/proposed/confronted by organizations? and Q2 – What are the different types of BDA methods theorized/proposed/employed to overcome BD challenges?. This systematic literature review (SLR) is carried out through observing and understanding the past trends and extant patterns/themes in the BDA research area, evaluating contributions, summarizing knowledge, thereby identifying limitations, implications and potential further research avenues to support the academic community in exploring research themes/patterns. Thus, to trace the implementation of BD strategies, a profiling method is employed to analyze articles (published in English-speaking peer-reviewed journals between 1996 and 2015) extracted from the Scopus database. The analysis presented in this paper has identified relevant BD research studies that have contributed both conceptually and empirically to the expansion and accrual of intellectual wealth to the BDA in technology and organizational resource management discipline.},
author = {Sivarajah, Uthayasankar and Kamal, Muhammad Mustafa and Irani, Zahir and Weerakkody, Vishanth},
doi = {10.1016/j.jbusres.2016.08.001},
file = {::},
isbn = {01482963 (ISSN)},
issn = {01482963},
journal = {Journal of Business Research},
keywords = {Big Data,Big Data Analytics,Challenges,Methods,Systematic literature review},
pages = {263--286},
publisher = {The Authors},
title = {{Critical analysis of Big Data challenges and analytical methods}},
url = {http://dx.doi.org/10.1016/j.jbusres.2016.08.001},
volume = {70},
year = {2017}
}
@article{Leimich2016,
abstract = {This paper discusses the challenges of performing a forensic investigation against a multi-node Hadoop cluster and proposes a methodology for examiners to use in such situations. The procedure's aim of minimising disruption to the data centre during the acquisition process is achieved through the use of RAM forensics. This affords initial cluster reconnaissance which in turn facilitates targeted data acquisition on the identified DataNodes. To evaluate the methodology's feasibility, a small Hadoop Distributed File System (HDFS) was configured and forensic artefacts simulated upon it by deleting data originally stored in the cluster. RAM acquisition and analysis was then performed on the NameNode in order to test the validity of the suggested methodology. The results are cautiously positive in establishing that RAM analysis of the NameNode can be used to pinpoint the data blocks affected by the attack, allowing a targeted approach to the acquisition of data from the DataNodes, provided that the physical locations can be determined. A full forensic analysis of the DataNodes was beyond the scope of this project.},
author = {Leimich, Petra and Harrison, Josh and Buchanan, William J.},
doi = {10.1016/j.diin.2016.07.003},
file = {::},
issn = {17422876},
journal = {Digital Investigation},
keywords = {Big data,Cloud storage forensics,Digital forensics,Distributed filesystem forensics,Hadoop forensics,RAM forensics,Triage},
pages = {96--109},
publisher = {Elsevier Ltd},
title = {{A RAM triage methodology for Hadoop HDFS forensics}},
url = {http://dx.doi.org/10.1016/j.diin.2016.07.003},
volume = {18},
year = {2016}
}
@article{VanBeek2015,
abstract = {The big data era has a high impact on forensic data analysis. Work is done in speeding up the processing of large amounts of data and enriching this processing with new techniques. Doing forensics calls for specific design considerations, since the processed data is incredibly sensitive. In this paper we explore the impact of forensic drivers and major design principles like security, privacy and transparency on the design and implementation of a centralized digital forensics service.},
author = {{Van Beek}, H. M.A. and {Van Eijk}, E. J. and {Van Baar}, R. B. and Ugen, M. and Bodde, J. N.C. and Siemelink, A. J.},
doi = {10.1016/j.diin.2015.07.004},
file = {::},
isbn = {9781466626621},
issn = {17422876},
journal = {Digital Investigation},
keywords = {Big data,Digital forensics,Distributed systems,Hansken,xiraf},
pages = {20--38},
publisher = {Elsevier Ltd},
title = {{Digital forensics as a service: Game on}},
url = {http://dx.doi.org/10.1016/j.diin.2015.07.004},
volume = {15},
year = {2015}
}
@article{Wagner2015,
abstract = {Abstract Forensic tools assist analysts with recovery of both the data and system events, even from corrupted storage. These tools typically rely on "file carving" techniques to restore files after metadata loss by analyzing the remaining raw file content. A significant amount of sensitive data is stored and processed in relational databases thus creating the need for database forensic tools that will extend file carving solutions to the database realm. Raw database storage is partitioned into individual "pages" that cannot be read or presented to the analyst without the help of the database itself. Furthermore, by directly accessing raw database storage, we can reveal things that are normally hidden from database users. There exists a number of database-specific tools developed for emergency database recovery, though not usually for forensic analysis of a database. In this paper, we present a universal tool that seamlessly supports many different databases, rebuilding table and other data content from any remaining storage fragments on disk or in memory. We define an approach for automatically (with minimal user intervention) reverse engineering storage in new databases, for detecting volatile data changes and discovering user action artifacts. Finally, we empirically verify our tool's ability to recover both deleted and partially corrupted data directly from the internal storage of different databases.},
author = {Wagner, James and Rasin, Alexander and Grier, Jonathan},
doi = {10.1016/j.diin.2015.05.013},
file = {::},
issn = {17422876},
journal = {Digital Investigation},
keywords = {Database forensics,Database storage modeling,File carving,Memory analysis,Stochastic analysis},
number = {S1},
pages = {S106--S115},
publisher = {Elsevier Ltd},
title = {{Database forensic analysis through internal structure carving}},
url = {http://dx.doi.org/10.1016/j.diin.2015.05.013},
volume = {14},
year = {2015}
}
@article{Mondek2017,
author = {Mondek, Dusan and Blazek, Rudolf B. and Zahradnicky, Tomas},
doi = {10.1109/QRS-C.2017.136},
file = {::},
isbn = {978-1-5386-2072-4},
journal = {2017 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)},
keywords = {a rt,business objectives,buzz-words,ii,information security systems,internet environment requires proactive,r eal -w orld,real-life,requirements,research collaboration,s tate of the,the hostility of the},
pages = {605--606},
title = {{Security Analytics in the Big Data Era}},
url = {http://ieeexplore.ieee.org/document/8004394/},
year = {2017}
}
@article{Gulzar2016,
author = {Gulzar, Muhammad Ali and Interlandi, Matteo and Yoo, Seunghyun and Tetali, Sai Deep and Condie, Tyson and Millstein, Todd and Kim, Miryung},
file = {::},
isbn = {9781450339001},
keywords = {able computing,big data analytics,data-intensive scal-,debugging,disc,fault localization and recovery,interactive tools},
title = {{Processing in Spark}},
year = {2016}
}
@article{Vlx,
author = {Vlx, Kdqj and Dqj, H Q R L and Xdqj, Klhq H Q J and Dxwkru, Fruuhvsrqglqj and Ghylfh, Vlqjoh Vwrudjh and Edvhg, W L V and Wkh, R Q and Wr, Q Rughu and Uhvsrqg, Idvw and Xvhu, W R and Iru, V Txhu and Surfhvvlqj, Gdwd and Rswlpl, W K H and Vwhp, H G V and Dxwrpdwlfdoo, F D Q and Lq, R U and Fdfkh, Glvn and Dfklhyhg, K D V and Wkh, I R U and Frppdqg, Uhshdwhg and Fdq, Txhulhv and Dfklhyhg, E H and Vkrzq, E Fuhdwlqj and Dssurdfk, Sursrvhg and Wkh, Lpsuryhv and Ri, Hiilflhqf and Elj, H Zrugv and Surfhvvlqj, Gdwd and Lq, Sodwirup and Fdfkh, Phpru and Surfhvvlqj, Gdwd and Wkdw, Sodwirupv and Eh, Zloo and Zlwk, Frpsdwleoh},
file = {::},
title = {{Esto esta mal}}
}
@article{Centre2016,
author = {Centre, Cyber Security and Jones, Andrew},
doi = {10.1109/ICDIM.2016.7829795},
file = {::},
isbn = {9781509026418},
keywords = {big data,cyber,digital forensics,informationsecurity,physical systems},
pages = {10--14},
title = {{Information Security and Digital Forensics in the world of Cyber Physical Systems}},
year = {2016}
}
@article{Adedayo2016,
abstract = {One of the main challenges in digital forensics is the increasing volume of data that needs to be analyzed. This problem has become even more pronounced with the emergence of big data and calls for a rethink on the way digital forensics investigations have been handled over the past years. This paper briefly discusses the challenges and needs of digital forensics in the face of the current trends and requirements of different investigations. A digital forensics analysis framework that puts into consideration the existing techniques as well as the current challenges is proposed. The purpose of the framework is to reassess the various stages of the digital forensics examination process and introduce into each stage the required techniques to enhance better collection, analysis, preservation and presentation in the face of big data and other challenges facing digital forensics.},
author = {Adedayo, Oluwasola Mary},
doi = {10.1109/ICCCF.2016.7740422},
file = {::},
isbn = {9781509060962},
journal = {Cybercrime and Computer Forensic (ICCCF), IEEE International Conference},
keywords = {although there are existing,big data,currently being followed in,digital forensics,digital forensics examination,forensics analysis framework,process,researcher,standards and process models,the digital forensics},
pages = {1--7},
title = {{Big data and digital forensics}},
year = {2016}
}
@inproceedings{Bhowmick2015,
author = {Bhowmick, Satyajit and Chakraborty, Suryadip and Agrawal, Dharma P.},
booktitle = {2015 IEEE 12th International Conference on Mobile Ad Hoc and Sensor Systems},
doi = {10.1109/MASS.2015.105},
isbn = {978-1-4673-9101-6},
month = {oct},
pages = {488--490},
publisher = {IEEE},
title = {{Study of Hadoop-MapReduce on Google N-Gram Datasets}},
url = {http://ieeexplore.ieee.org/document/7366980/},
year = {2015}
}
@inproceedings{Stocia2015,
author = {Stocia, Ion},
booktitle = {2015 IEEE International Conference on Big Data (Big Data)},
doi = {10.1109/BigData.2015.7363734},
file = {:C$\backslash$:/Users/Hp/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stocia - 2015 - Conquering Big Data with Spark.pdf:pdf},
isbn = {978-1-4799-9926-2},
month = {oct},
pages = {3--3},
publisher = {IEEE},
title = {{Conquering Big Data with Spark}},
url = {http://ieeexplore.ieee.org/document/7363734/},
year = {2015}
}
@inproceedings{Yuan2016,
author = {Yuan, Yuan and Salmi, Meisam Fathi and Huai, Yin and Wang, Kaibo and Lee, Rubao and Zhang, Xiaodong},
booktitle = {2016 IEEE International Conference on Big Data (Big Data)},
doi = {10.1109/BigData.2016.7840613},
file = {:C$\backslash$:/Users/Hp/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yuan et al. - 2016 - Spark-GPU An accelerated in-memory data processing engine on clusters.pdf:pdf},
isbn = {978-1-4673-9005-7},
month = {dec},
pages = {273--283},
publisher = {IEEE},
title = {{Spark-GPU: An accelerated in-memory data processing engine on clusters}},
url = {http://ieeexplore.ieee.org/document/7840613/},
year = {2016}
}
@inproceedings{Chang2016,
author = {Chang, Bao Rong and Tsai, Hsiu-Fen and Wang, Yo-Ai and Huang, Chien-Feng},
booktitle = {2016 International Conference on Applied System Innovation (ICASI)},
doi = {10.1109/ICASI.2016.7539859},
isbn = {978-1-4673-9888-6},
month = {may},
pages = {1--4},
publisher = {IEEE},
title = {{Resilient distributed computing platforms for big data analysis using Spark and Hadoop}},
url = {http://ieeexplore.ieee.org/document/7539859/},
year = {2016}
}
@inproceedings{Chen2017,
author = {Chen, Chao and Yan, Yuzhong and Huang, Lei and Qian, Lijun},
booktitle = {2017 New York Scientific Data Summit (NYSDS)},
doi = {10.1109/NYSDS.2017.8085038},
file = {:C$\backslash$:/Users/Hp/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2017 - Implementing a distributed volumetric data analytics toolkit on apache spark.pdf:pdf},
isbn = {978-1-5386-3161-4},
month = {aug},
pages = {1--8},
publisher = {IEEE},
title = {{Implementing a distributed volumetric data analytics toolkit on apache spark}},
url = {http://ieeexplore.ieee.org/document/8085038/},
year = {2017}
}
@inproceedings{Mishra2016,
author = {Mishra, Prakhar and Garg, Ratika and Kumar, Akshat and Gupta, Arpan and Kumar, Praveen},
booktitle = {2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI)},
doi = {10.1109/ICACCI.2016.7732275},
file = {:C$\backslash$:/Users/Hp/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mishra et al. - 2016 - Song year prediction using Apache Spark.pdf:pdf},
isbn = {978-1-5090-2029-4},
month = {sep},
pages = {1590--1594},
publisher = {IEEE},
title = {{Song year prediction using Apache Spark}},
url = {http://ieeexplore.ieee.org/document/7732275/},
year = {2016}
}
@inproceedings{Wang2016,
author = {Wang, Wenjuan and Liu, Taoying and Tang, Dixin and Liu, Hong and Li, Wei and Lee, Rubao},
booktitle = {2016 IEEE International Conference on Networking, Architecture and Storage (NAS)},
doi = {10.1109/NAS.2016.7549422},
file = {:C$\backslash$:/Users/Hp/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2016 - SparkArray An Array-Based Scientific Data Management System Built on Apache Spark.pdf:pdf},
isbn = {978-1-5090-3315-7},
month = {aug},
pages = {1--10},
publisher = {IEEE},
title = {{SparkArray: An Array-Based Scientific Data Management System Built on Apache Spark}},
url = {http://ieeexplore.ieee.org/document/7549422/},
year = {2016}
}
@inproceedings{Tahir2015,
author = {Tahir, Shahzaib and Iqbal, Waseem},
booktitle = {2015 First International Conference on Anti-Cybercrime (ICACC)},
doi = {10.1109/Anti-Cybercrime.2015.7351932},
file = {:C$\backslash$:/Users/Hp/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tahir, Iqbal - 2015 - Big Data — An evolving concern for forensic investigators.pdf:pdf},
isbn = {978-1-4799-7620-1},
month = {nov},
pages = {1--6},
publisher = {IEEE},
title = {{Big Data — An evolving concern for forensic investigators}},
url = {http://ieeexplore.ieee.org/document/7351932/},
year = {2015}
}
@article{Fu2017,
author = {Fu, Xiao and Gao, Yun and Luo, Bin and Du, Xiaojiang and Guizani, Mohsen},
doi = {10.1109/MNET.2017.1500095NM},
file = {:C$\backslash$:/Users/Hp/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fu et al. - 2017 - Security Threats to Hadoop Data Leakage Attacks and Investigation.pdf:pdf},
issn = {0890-8044},
journal = {IEEE Network},
month = {mar},
number = {2},
pages = {67--71},
title = {{Security Threats to Hadoop: Data Leakage Attacks and Investigation}},
url = {http://ieeexplore.ieee.org/document/7827929/},
volume = {31},
year = {2017}
}
@inproceedings{Shenwen2015,
author = {Shenwen, Lin and Yingbo, Li and Xiongjie, Du},
booktitle = {2015 IEEE 5th International Conference on Electronics Information and Emergency Communication},
doi = {10.1109/ICEIEC.2015.7284547},
file = {:C$\backslash$:/Users/Hp/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shenwen, Yingbo, Xiongjie - 2015 - Study and research of APT detection technology based on big data processing architecture.pdf:pdf},
isbn = {978-1-4799-7283-8},
month = {may},
pages = {313--316},
publisher = {IEEE},
title = {{Study and research of APT detection technology based on big data processing architecture}},
url = {http://ieeexplore.ieee.org/document/7284547/},
year = {2015}
}
@inproceedings{Zawoad2015,
author = {Zawoad, Shams and Hasan, Ragib},
booktitle = {2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems},
doi = {10.1109/HPCC-CSS-ICESS.2015.305},
file = {:C$\backslash$:/Users/Hp/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zawoad, Hasan - 2015 - Digital Forensics in the Age of Big Data Challenges, Approaches, and Opportunities.pdf:pdf},
isbn = {978-1-4799-8937-9},
month = {aug},
pages = {1320--1325},
publisher = {IEEE},
title = {{Digital Forensics in the Age of Big Data: Challenges, Approaches, and Opportunities}},
url = {http://ieeexplore.ieee.org/document/7336350/},
year = {2015}
}
@inproceedings{Agrawal2016,
author = {Agrawal, Bikash and Hansen, Raymond and Rong, Chunming and Wiktorski, Tomasz},
booktitle = {2016 IEEE International Congress on Big Data (BigData Congress)},
doi = {10.1109/BigDataCongress.2016.30},
file = {:C$\backslash$:/Users/Hp/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Agrawal et al. - 2016 - SD-HDFS Secure Deletion in Hadoop Distributed File System.pdf:pdf},
isbn = {978-1-5090-2622-7},
month = {jun},
pages = {181--189},
publisher = {IEEE},
title = {{SD-HDFS: Secure Deletion in Hadoop Distributed File System}},
url = {http://ieeexplore.ieee.org/document/7584936/},
year = {2016}
}
@inproceedings{Jones2016,
author = {Jones, Andrew and Vidalis, Stilianos and Abouzakhar, Nasser},
booktitle = {2016 Eleventh International Conference on Digital Information Management (ICDIM)},
doi = {10.1109/ICDIM.2016.7829795},
isbn = {978-1-5090-2641-8},
month = {sep},
pages = {10--14},
publisher = {IEEE},
title = {{Information security and digital forensics in the world of cyber physical systems}},
url = {http://ieeexplore.ieee.org/document/7829795/},
year = {2016}
}
@inproceedings{Mondek2017a,
author = {Mondek, Dusan and Blazek, Rudolf B. and Zahradnicky, Tomas},
booktitle = {2017 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)},
doi = {10.1109/QRS-C.2017.136},
isbn = {978-1-5386-2072-4},
month = {jul},
pages = {605--606},
publisher = {IEEE},
title = {{Security Analytics in the Big Data Era}},
url = {http://ieeexplore.ieee.org/document/8004394/},
year = {2017}
}
@article{Sivarajah2017a,
abstract = {a b s t r a c t Big Data (BD), with their potential to ascertain valued insights for enhanced decision-making process, have re-cently attracted substantial interest from both academics and practitioners. Big Data Analytics (BDA) is increas-ingly becoming a trending practice that many organizations are adopting with the purpose of constructing valuable information from BD. The analytics process, including the deployment and use of BDA tools, is seen by organizations as a tool to improve operational efficiency though it has strategic potential, drive new revenue streams and gain competitive advantages over business rivals. However, there are different types of analytic ap-plications to consider. Therefore, prior to hasty use and buying costly BD tools, there is a need for organizations to first understand the BDA landscape. Given the significant nature of the BD and BDA, this paper presents a state-of-the-art review that presents a holistic view of the BD challenges and BDA methods theorized/proposed/ employed by organizations to help others understand this landscape with the objective of making robust invest-ment decisions. In doing so, systematically analysing and synthesizing the extant research published on BD and BDA area. More specifically, the authors seek to answer the following two principal questions: Q1 – What are the different types of BD challenges theorized/proposed/confronted by organizations? and Q2 – What are the different types of BDA methods theorized/proposed/employed to overcome BD challenges?. This systematic literature review (SLR) is carried out through observing and understanding the past trends and extant patterns/themes in the BDA research area, evaluating contributions, summarizing knowledge, thereby identifying limitations, implica-tions and potential further research avenues to support the academic community in exploring research themes/patterns. Thus, to trace the implementation of BD strategies, a profiling method is employed to analyze articles (published in English-speaking peer-reviewed journals between 1996 and 2015) extracted from the Scopus database. The analysis presented in this paper has identified relevant BD research studies that have contributed both conceptually and empirically to the expansion and accrual of intellectual wealth to the BDA in technology and organizational resource management discipline.},
author = {Sivarajah, Uthayasankar and Kamal, Muhammad Mustafa and Irani, Zahir and Weerakkody, Vishanth},
doi = {10.1016/j.jbusres.2016.08.001},
file = {:C$\backslash$:/Users/Hp/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sivarajah et al. - 2017 - Critical analysis of Big Data challenges and analytical methods.pdf:pdf},
journal = {Journal of Business Research},
keywords = {Big Data,Big Data Analytics,Challenges,Methods,Systematic literature review},
pages = {263--286},
title = {{Critical analysis of Big Data challenges and analytical methods}},
url = {https://ac-els-cdn-com.salford.idm.oclc.org/S014829631630488X/1-s2.0-S014829631630488X-main.pdf?{\_}tid=ee71208c-be5f-11e7-84aa-00000aacb35d{\&}acdnat=1509470669{\_}f46e43835f0d6e2ab6def925c7ce7aea},
volume = {70},
year = {2017}
}
@article{Meng2015,
abstract = {Apache Spark is a popular open-source platform for large-scale data processing that is well-suited for iterative machine learning tasks. In this paper we present MLlib, Spark's open-source distributed machine learning library. MLlib provides efficient functionality for a wide range of learning settings and includes several underlying statistical, optimization, and linear algebra primitives. Shipped with Spark, MLlib supports several languages and provides a high-level API that leverages Spark's rich ecosystem to simplify the development of end-to-end machine learning pipelines. MLlib has experienced a rapid growth due to its vibrant open-source community of over 140 contributors, and includes extensive documentation to support further growth and to let users quickly get up to speed.},
archivePrefix = {arXiv},
arxivId = {1505.06807},
author = {Meng, Xiangrui and Bradley, Joseph and Yavuz, Burak and Sparks, Evan and Venkataraman, Shivaram and Liu, Davies and Freeman, Jeremy and Tsai, DB and Amde, Manish and Owen, Sean and Xin, Doris and Xin, Reynold and Franklin, Michael J. and Zadeh, Reza and Zaharia, Matei and Talwalkar, Ameet},
doi = {10.1145/2882903.2912565},
eprint = {1505.06807},
file = {::},
isbn = {9781450335317},
issn = {1532-4435},
pmid = {25246403},
title = {{MLlib: Machine Learning in Apache Spark}},
year = {2015}
}
@article{Bharill,
abstract = {—A huge amount of digital data containing useful information, called Big Data, is generated everyday. To mine such useful information, clustering is widely used data analysis technique. A large number of Big Data analytics frameworks have been developed to scale the clustering algorithms for big data analysis. One such framework called Apache Spark works really well for iterative algorithms by supporting in-memory computations, scalability etc. We focus on the design and implementation of partitional based clustering algorithms on Apache Spark, which are suited for clustering large datasets due to their low computational requirements. In this paper, we propose Scalable Random Sampling with Iterative Optimization Fuzzy c-Means algorithm (SRSIO-FCM) implemented on an Apache Spark Cluster to handle the challenges associated with big data clustering. Experimental studies on various big datasets have been conducted. The performance of SRSIO-FCM is judged in comparison with the proposed scalable version of the Literal Fuzzy c-Means (LFCM) and Random Sampling plus Extension Fuzzy c-Means (rseFCM) implemented on the Apache Spark cluster. The comparative results are reported in terms of time and space complexity, run time and measure of clustering quality, showing that SRSIO-FCM is able to run in much less time without compromising the clustering quality.},
author = {Bharill, Neha and Tiwari, Aruna and Malviya, Aayushi},
doi = {10.1109/TBDATA.2016.2622288},
file = {::},
keywords = {Apache Spark,Index Terms—Fuzzy clustering,big data,distributed machine learning,iterative computation},
title = {{Fuzzy Based Scalable Clustering Algorithms for Handling Big Data Using Apache Spark}}
}
@article{Interlandi,
abstract = {Debugging data processing logic in Data-Intensive Scalable Computing (DISC) systems is a difficult and time consum-ing effort. Today's DISC systems offer very little tooling for debugging programs, and as a result programmers spend countless hours collecting evidence (e.g., from log files) and performing trial and error debugging. To aid this effort, we built Titian, a library that enables data provenance— tracking data through transformations—in Apache Spark. Data scientists using the Titian Spark extension will be able to quickly identify the input data at the root cause of a po-tential bug or outlier result. Titian is built directly into the Spark platform and offers data provenance support at interactive speeds—orders-of-magnitude faster than alterna-tive solutions—while minimally impacting Spark job perfor-mance; observed overheads for capturing data lineage rarely exceed 30{\%} above the baseline job execution time.},
author = {Interlandi, Matteo and Shah, Kshitij and Deep, Sai and Muhammad, Tetali and Gulzar, Ali and Yoo, Seunghyun and Kim, Miryung and Millstein, Todd and Condie, Tyson},
file = {::},
title = {{Titian: Data Provenance Support in Spark}}
}
@article{Huang2017,
abstract = {MapReduce has been widely used in Hadoop for parallel processing larger-scale data for the last decade. However, remote-sensing (RS) algorithms based on the programming model are trapped in dense disk I/O operations and unconstrained network communication, and thus inappropriate for timely processing and analyzing massive, heterogeneous RS data. In this paper, a novel in-memory computing framework called Apache Spark (Spark) is introduced. Through its merits of transferring transformation to in-memory datasets of Spark, the shortages are eliminated. To facilitate implementation and assure high performance of Spark-based algorithms in a complex cloud computing environment, a strip-oriented parallel programming model is proposed. By incorporating strips of RS data with resilient distributed datasets (RDDs) of Spark, all-level parallel RS algorithms can be easily expressed with coarse-grained transformation primitives and BitTorrent-enabled broadcast variables. Additionally, a generic image partition method for Spark-based RS algorithms to efficiently generate differentiable key/value strips from a Hadoop distributed file system (HDFS) is implemented for concealing the heterogeneousness of RS data. Data-intensive multitasking algorithms and iteration-intensive algorithms were evaluated on a Hadoop yet another resource negotiator (YARN) platform. Experiments indicated that our Spark-based parallel algorithms are of great efficiency, a multitasking algorithm took less than 4 h to process more than half a terabyte of RS data on a small YARN cluster, and 9*9 convolution operations against a 909-MB image took less than 260 s. Further, the efficiency of iteration-intensive algorithms is insensitive to image size.},
author = {Huang, Wei and Meng, Lingkui and Zhang, Dongying and Zhang, Wen},
doi = {10.1109/JSTARS.2016.2547020},
file = {::},
isbn = {1939-1404 VO - 10},
issn = {21511535},
journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
keywords = {Apache Spark,Hadoop yet another resource negotiator (YARN),big data,parallel processing,remote sensing (RS)},
title = {{In-Memory Parallel Processing of Massive Remotely Sensed Data Using an Apache Spark on Hadoop YARN Model}},
year = {2017}
}
@article{,
file = {:C$\backslash$:/Users/Hp/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Apache Spark a unified engine for big data processin.pdf:pdf},
title = {{Apache Spark a unified engine for big data processin}}
}
@article{Islam2016,
author = {Islam, Nusrat Sharmin and Wasi-Ur-Rahman, Md and Lu, Xiaoyi and Panda, Dhabaleswar K.D.K.},
doi = {10.1109/BigData.2016.7840608},
file = {::},
isbn = {9781467390040},
journal = {Proceedings - 2016 IEEE International Conference on Big Data, Big Data 2016},
pages = {223--232},
title = {{Efficient data access strategies for Hadoop and Spark on HPC cluster with heterogeneous storage}},
year = {2016}
}
@article{Vaquero2015,
abstract = {Public clouds have democratised the access to analytics for virtually any institution in the world. Virtual Machines (VMs) can be provisioned on demand, and be used to crunch data after uploading into the VMs. While this task is trivial for a few tens of VMs, it becomes increasingly complex and time consuming when the scale grows to hundreds or thousands of VMs crunching tens or hundreds of TB. Moreover, the elapsed time comes at a price: the cost of provisioning VMs in the cloud and keeping them waiting to load the data. In this paper we present a big data provisioning service that incorporates hierarchical and peer-to-peer data distribution techniques to speed-up data loading into the VMs used for data processing. The system dynamically mutates the sources of the data for the VMs to speed-up data loading. We tested this solution with 1000 VMs and 100 TB of data, reducing time by at least 30 {\%} over current state of the art techniques. This dynamic topology mechanism is tightly coupled with classic declarative machine configuration techniques (the system takes a single high-level declarative configuration file and configures both software and data loading). Together, these two techniques simplify the deployment of big data in the cloud for end users who may not be experts in infrastructure management.},
author = {Vaquero, Luis M. and Celorio, Antonio and Cuadrado, Felix and Cuevas, Ruben},
doi = {10.1109/TCC.2014.2360376},
file = {::},
issn = {21687161},
journal = {IEEE Transactions on Cloud Computing},
keywords = {BitTorrent,Large-scale data transfer,big data,big data distribution,flash crowd,p2p overlay,provisioning},
number = {2},
pages = {132--144},
title = {{Deploying large-scale datasets on-demand in the cloud: Treats and tricks on data distribution}},
volume = {3},
year = {2015}
}
@article{Fetjah2016,
abstract = {Cloud Computing did come up with so many attractive advantages such as scalability, flexibility, accessibility, rapid application deployment, and user self service. However in hindsight, Cloud Computing makes ensuring security within these environments so much challenging. Therefore traditional security mechanisms such as firewalls and antivirus softwares have proven insufficient and incapable of dealing with the sheer amount of data and events generated within a Cloud infrastructure. Herein, we present a highly scalable module based system that relies upon Big Data techniques and tools providing a comprehensive solution to process and analyze relevant events (packets flow, logs files) in order to generate an informative decisions that will be handled accordingly and swiftly.},
author = {Fetjah, Laila and Benzidane, Karim and Alloussi, Hassan El and Warrak, Othman El and Jai-Andaloussi, Said and Sekkaki, Abderrahim},
doi = {10.1109/CSCloud.2016.53},
file = {::},
isbn = {9781509009459},
journal = {Proceedings - 3rd IEEE International Conference on Cyber Security and Cloud Computing, CSCloud 2016 and 2nd IEEE International Conference of Scalable and Smart Cloud, SSC 2016},
keywords = {Big Data,Cloud Computing,Hadoop,ITIL,SKMS,Security Information and Event Management (SIEM),Security Intelligence,Spark},
pages = {190--197},
title = {{Toward a Big Data Architecture for Security Events Analytic}},
year = {2016}
}
@article{Zeng2015,
abstract = {Nearly years ago, Hellerstein, Haas and Wang proposed online aggregation (OLA), a technique that allows users to (() observe the progress of a query by showing iteratively reened approximate an-swers, and (() stop the query execution once its result achieves the desired accuracy. In this demonstration, we present G-OLA, a novel mini-batch execution model that generalizes OLA to support gen-eral OLAP queries with arbitrarily nested aggregates using eecient delta maintenance techniques. We have implemented G-OLA in FluoDB, a parallel online query execution framework that is built on top of the Spark cluster computing framework that can scale to massive data sets. We will demonstrate FluoDB on a cluster of machines processing roughly of real-world session logs from a video-sharing website. Using an ad optimization and an A/B test-ing based scenario, we will enable users to perform real-time data analysis via web-based query consoles and dashboards.},
author = {Zeng, Kai and Agarwal, Sameer and Dave, Ankur and Armbrust, Michael and Stoica, Ion},
doi = {10.1145/2723372.2735381},
file = {::},
isbn = {9781450327589},
issn = {07308078},
journal = {Sigmod 2015},
pages = {913--918},
title = {{G-OLA : Generalized On-Line Aggregation for Interactive Analysis on Big Data}},
url = {http://www.eecs.berkeley.edu/{~}kaizeng/papers/sigmod{\_}2015.pdf},
year = {2015}
}
